# Load data

## Administrative units

We has performed the analysis in Copenhagen (*KOMKODE = 0101*) and Frederiksberg (*KOMKODE = 0147*) communes. For adding new communes we would only need to add the code in the following chunk and the analysis will be automatically updated when running again the analysis. 
```{r commune-codes, cache=TRUE}

  study_area_codes <- c("0101", "0147")

```

Load shapefile with the communes polygons of Denmark, and select those in the study area:
```{r shp-commune, cache = TRUE, dependson = "commune-codes"}

  commune_link <- paste(loc_dir,
                        "DAGIREF_SHAPE_UTM32-EUREF89/ADM",
                        "KOMMUNE.shp",
                        sep = "/")
  dk_commune  <- read_sf(commune_link) %>%
    st_zm() %>% 
    st_transform(crs = "EPSG:25832")
  
  cph_commune <- filter(dk_commune, KOMKODE %in% study_area_codes)

```

Load shapefile with the parishes polygons of Denmark, and select those in the study area: 
```{r shp-parish, cache = TRUE, dependson = "commune-codes"}

  parish_link <- paste(loc_dir, "DAGIREF_SHAPE_UTM32-EUREF89/ADM", "SOGN.shp", sep = "/")
  dk_parish  <- read_sf(parish_link) %>%
    st_zm() %>% 
    st_transform(crs = "EPSG:25832")

# Select those where the centroid is in the study area
  dk_parish_cent <- st_centroid(dk_parish)
  cph_parish_cent <- st_intersection(dk_parish_cent, cph_commune)
  
  cph_parish <- filter(dk_parish, SOGNEKODE %in% cph_parish_cent$SOGNEKODE) %>% 
    # Combine several parish features geometries into one polygon
    group_by(SOGNEKODE, SOGNENAVN) %>% 
    summarise(geometry = st_union(geometry)) %>% 
    ungroup() %>% 
    # add area of the parish (in km2)
    mutate(prsh_area_km2 = as.numeric(units::set_units(st_area(.), km^2)))

```

Contour of the study area (merge the parishes in one polygon):
```{r study-area, cache = TRUE, dependson = "commune-codes"}

  study_area <- cph_parish %>%
    st_union() %>%
    st_sf() %>% 
    st_transform(crs = "EPSG:25832")

```

Make grids cells of 100 m x 100 m over the study area:
```{r make-grids, cache = TRUE, dependson = "commune-codes"}

  grids100m <- study_area %>% 
    # Make regular grids (100m x 100m)
    st_make_grid(cellsize = 100) %>% 
    st_sf() %>% 
    # Name grids as "g001", "g002", ...
    mutate(grid_ID = paste0("g", stringr::str_pad(seq(1, nrow(.), 1), 3, pad = "0")))

```

## BBR data    

Residential units codes (BYG_ANV - Buildings for year-round living):

- 120, 121, 122 = Detached single-family house (detached house). 
- 130, 131, 132 = Townhouse, chain, or semi-detached house (vertical separation between the units).
- 140           = Multi-storey residential building (multi-family house, including two-family house (horizontal separation between the units).
- 190           = Second building for year-round living (NOT USED - VERY LOW NUMBER)

```{r res-codes}

# Residential houss codes
  res_codes <- tribble (~BYG_ANVEND_KODE, ~type,
                        120, "Single-family house",
                        121, "Single-family house",
                        122, "Single-family house",
                        130, "Row, chain, or semi-detached house",
                        131, "Row, chain, or semi-detached house",
                        132, "Row, chain, or semi-detached house",
                        140, "Multi-storey residential building",
                        190, "Second building for year-round living") %>%
    # Convert type to factor
    mutate(type = factor(type,
                         levels = c("Single-family house",
                                    "Multi-storey residential building",
                                    "Row, chain, or semi-detached house",
                                    "Second building for year-round living")))

```

Function for reading residential units from a csv file:
```{r aux-funct-BBR}

#' Read residential units from BBR data 
#' Get read residential units in an area
#' @param file Link to OneDrive with the data (.csv format)
#' @param area Area where we would get the data (default = study_area)

  f_res_units  <- function(.file, .area = study_area) { 
    fread(.file) %>% 
      # Convert to tibble
      as_tibble() %>% 
      # Select only Residential houses - Buildings for year-round living 
      filter(ENH_ANVEND_KODE %in% res_codes$BYG_ANVEND_KODE) %>% 
      # Input empty cells (buildings with only one floor) in Etagebetegn as "st"
      mutate(Etagebetegn = ifelse(Etagebetegn == "", "st", Etagebetegn),
             # Etagebetegn as ordened factor
             Etagebetegn = factor(Etagebetegn, c("k2", "kl", "st", seq(1, 36, 1)),
                                  ordered = TRUE),
             # Group floor levels with 5 or more
             floor_level = fct_other(Etagebetegn,
                                     drop = factor(seq(5, 36)),
                                     other_level = "5 or more")) %>%
      # Add residential description (type) into the dataset
      left_join(res_codes, by = c("ENH_ANVEND_KODE" = "BYG_ANVEND_KODE")) %>% 
      # Convert to sf objects
      st_as_sf(coords = c("etrs89koordinat_ost", "etrs89koordinat_nord"),
               crs = "EPSG:25832") %>% 
      # Get only points in the study area
      mutate(int = st_intersects(., .area) %>% lengths > 0) %>% 
      filter(int == TRUE) %>% 
      # Add year of the BBR dataset  
      mutate(BBR_year = parse_number(stringr::str_extract(.file, "_[0-9]+_")))
  }

```

Load all csv files (one file for each year) in the same tibble with a column indicating the year of the dataset: 
```{r load-BBR, cache=TRUE, dependson = "commune-codes", cache.extra = file.mtime('C:/Users/FU53VP/OneDrive - Aalborg Universitet/BBR_2021/csv/')}

# Load residential units 
# NOTE: you may need to change the path to OneDrive - Aalborg Universitet 
# Read all the data together 
  OneDrive_link <- "C:/Users/FU53VP/OneDrive - Aalborg Universitet/BBR_2021/csv/"
  csv_files_link <- list.files(path = OneDrive_link, pattern = "*.csv", full.names = TRUE)
  
  res_units <- future_map_dfr(.x = csv_files_link, .f = f_res_units)

```

## Population data 

Population by parish, with ancestry (Danish origin and immigrants from western and non-western countries). Two tables from DTS:

- KMSTA001: Population 1. January by parish, ancestry and National Church
- KMSTA003: Summary vital statistics by parish and movements

We firs create auxiliary functions for reading the data with the package *danstat*:
```{r aux-funct-DST}

# Loop by year for getting DST data) 
  steps <- function(year){
    var_values <- list(id_region, id_ancestry, year)
    var_input <- purrr::map2(.x = var_codes,
                             .y = var_values,
                             .f = ~list(code = .x, values = .y))
    get_data(id_table, variables = var_input)
  }

# Remove punctuation, lowercase, stem, stopwords, and collapse strings
  rm_words <- function(x, stopwords) { x %>% 
      strsplit(" ", fixed = TRUE) %>% 
      lapply(tm::removePunctuation) %>% 
      lapply(tolower) %>% 
      lapply(SnowballC::wordStem) %>% 
      lapply(function(x) x[!x %in% stopwords]) %>% 
      vapply(function(x) paste(x , collapse = "_"), character(1))
  }

```

Read and clean the first table (KMSTA001):
```{r load-KMSTA001, cache = TRUE, dependson = "shp-parish"}

  id_table <- "KMSTA001"
  var_pop <- get_table_metadata(table_id = id_table, variables_only = TRUE)

# Codes for var_input
  var_codes <- c("SOGN", "HERKOMST", "Tid")

# Values for var_input
  # Region: parishes of the study area (i.e. cph_parish)
  id_region <- cph_parish$SOGNEKODE
  # Ancestry
  id_ancestry <- NA
  # Quarters
  id_year <- var_pop$values[[4]]$id   # Select all years
  
# Read data
  prsh_ances_dst <- id_year %>%
    future_map(steps) %>% 
    bind_rows()
  plan("default")

# Clean data 
  prsh_ances <- prsh_ances_dst %>% 
    # Translate column names into English
    rename(parish = SOGN,
           ancestry = HERKOMST,
           date = TID, 
           value = INDHOLD) %>% 
    # Get parish codes (first number of the string)
    mutate(prsh_id = stringr::str_extract(parish, "[[:alnum:]]*")) %>% 
    # Remove the code from the string (prsh)
    mutate(parish = sub("[[:alnum:]]* ", "", parish)) %>% 
    # Get municipality (info inside the parenthesis)
    mutate(prsh_muni = stringr::str_extract(parish, "(?<=\\().*(?=\\))"),
           prsh_muni = gsub(" Municipality", "", prsh_muni),
           # removes white space from start and end of string
           prsh_muni = stringr::str_trim(prsh_muni)) %>% 
    # Get parish name (outside parenthesis) 
    mutate(prsh_name = stringr::str_extract(parish, "(.*(?=\\())"),
           prsh_name = stringr::str_trim(prsh_name)) %>% 
    # Remove duplicate info and order columns 
    select(prsh_id, prsh_name, prsh_muni, date, ancestry, value) %>% 
    # Make shorter names in ancestry
    mutate(ancestry = case_when(
      ancestry == "Persons of Danish origin" ~ "pop_dan",
      ancestry == "Immigrants from western countries" ~ "pop_mi_wst",
      ancestry == "Immigrants from non-western countries" ~ "pop_mi_nwst",
      ancestry == "Descendants from western countries" ~ "pop_de_wst",
      ancestry == "Descendants from non-western countries" ~ "pop_de_nwst"), 
      ancestry = factor(ancestry)) %>% 
    # Pivot (one row for peach parish and year)
    pivot_wider(names_from = ancestry, values_from = value) %>% 
    # Merge immigrants and their descendants (i.e. foreigners) 
    mutate(pop_frgn_wst = pop_mi_wst + pop_de_wst, 
           pop_frgn_nwst = pop_mi_nwst + pop_de_nwst) %>% 
    select(-c(pop_mi_wst, pop_de_wst, pop_mi_nwst, pop_de_nwst)) %>% 
    # Add column with total population
    mutate(pop_total = select(., starts_with("pop_")) %>% rowSums())
  
```

Read and clean second table (KMSTA003):
```{r load-KMSTA003, cache = TRUE, dependson = "shp-parish"}

  id_table <- "KMSTA003"
  var_pop <- get_table_metadata(table_id = id_table, variables_only = TRUE)

# Codes for var_input
  var_codes <- c("SOGN", "KIRKEBEV", "Tid")

# Values for var_input
  # Region: all parish
  id_region <- cph_parish$SOGNEKODE
  # Ancestry
  id_movements <- NA
  # Quarters
  id_year <- var_pop$values[[3]]$id   # Select all years

# Read data
  plan(multisession)  
  prsh_stats_dst <- id_year %>%
    future_map(steps) %>% 
    bind_rows()
  plan("default")

# Clean data 
  prsh_stats <- prsh_stats_dst %>%
    # Translate column names into English
    rename(parish = SOGN,
           movements = KIRKEBEV,
           date = TID, 
           value = INDHOLD) %>% 
    # Get parish codes (first number of the string)
    mutate(prsh_id = stringr::str_extract(parish, "[[:alnum:]]*")) %>% 
    # Remove the code from the string (prsh)
    mutate(parish = sub("[[:alnum:]]* ", "", parish)) %>% 
    # Get municipality (info inside the parenthesis)
    mutate(prsh_muni = stringr::str_extract(parish, "(?<=\\().*(?=\\))"),
           prsh_muni = gsub(" Municipality", "", prsh_muni),
           # removes white space from start and end of string
           prsh_muni = stringr::str_trim(prsh_muni)) %>% 
    # Get parish name (outside parenthesis) 
    mutate(prsh_name = stringr::str_extract(parish, "(.*(?=\\())"),
           prsh_name = stringr::str_trim(prsh_name)) %>% 
    # remove duplicate info and order columns 
    select(prsh_id, prsh_name, prsh_muni, date, movements, value) %>% 
    # Clean arguments in movements (remove punctuation, stop-words, stem, and collapse)
    mutate(movements = rm_words(movements, c("in", "the", "of"))) %>%
    # Pivot (one row for each parish and year) 
    pivot_wider(names_from = movements, values_from = value)
  
```

Merge both tables in one:
```{r merge-DST, cache = TRUE, dependson = "shp-parish"}

  prsh_pop <- prsh_ances %>%  
    full_join(prsh_stats) %>% 
    # remove rows with no summary data (summary data only from 2015 to 2020)
    filter(date >= 2015, date <= 2020)

```

Add the spatial information:
```{r spatial-DST,  cache = TRUE, dependson = "shp-parish"}
  
  prsh_pop_sf <- cph_parish %>%
    rename(prsh_id = SOGNEKODE) %>% 
    select(prsh_id, prsh_area_km2) %>% 
    left_join(prsh_pop, by = c("prsh_id")) 

```

##  CORINE land use 

Create a table with the CORINE land use codes from the EEA
```{r CORINE-link}
 
  corine_code_link <- "https://www.eea.europa.eu/data-and-maps/data/corine-land-cover-2000-clc2000-250-m-version-9-2007/corine-land-cover-2000-classes-and-rgb-color-codes/clc2000legend.csv/at_download/file"

  corine_code <- read_csv(corine_code_link) %>% 
    mutate_if(is.numeric, as.character)
  
```

Load the shapefile downloaded from kortforsyningen into the local directory.
```{r shp-CORINE, cache=TRUE, dependson="commune-codes"}

# Local directory
  dangeo_set_param()

# Load shape fine
  corine_link <- paste(loc_dir, "DK_CORINE_SHP_UTM32-WGS84", "CLC12_DK.shp", sep = "/")
  dk_corine  <- read_sf(corine_link) %>%
    # Drop z dimension
    st_zm() %>% 
    # Transform coordinates
    st_transform(crs = "EPSG:25832") %>% 
    # Add code description
    left_join(corine_code, by = c("CODE_12" = "CLC_CODE")) 
  
# Select study area
  cph_corine <- st_intersection(dk_corine, study_area) %>% 
    select(CODE_12, LABEL1, LABEL2, LABEL3 , RGB) %>% 
    janitor::clean_names() %>% 
    separate(rgb, c("red", "green", "blue"), sep = "-") %>% 
    mutate(code_12 = as.numeric(code_12),
           hex_col = grDevices::rgb(red, green, blue, max = 255),
           hex_col = fct_reorder(hex_col, code_12),
           label3 = fct_reorder(label3, code_12)) %>% 
    st_sf()
  
```
