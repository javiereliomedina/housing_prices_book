# Data

## Administrative units

The Denmark's Administrative Geographical Division ([DAGI](https://sdfe.dk/hent-data/danmarks-administrative-geografiske-inddeling/)) has been used for obtaining the administrative boundaries of Denmark. In this sense, the country is divided in approx. 2200 parishes, 98 municipalities, 5 regions, 22 judicial districts, 12 police districts, 92 constituencies and approx. 1100 postcodes. However, we focused our study in the Copenhagen (*KOMKODE = 0101*) and Frederiksberg (*KOMKODE = 0147*) communes, and we get the statistics for neighbourhood characteristics from data at at parish level.    

```{r adm-units, cache=TRUE}

## Codes of the comunes under study (KOMKODE)
study_area_codes <- c("0101", "0147")

## Communes polygons of Denmark, and select those in the study area
commune_link <- paste(loc_dir, "DAGIREF_SHAPE_UTM32-EUREF89/ADM", "KOMMUNE.shp", sep = "/")
dk_commune  <- read_sf(commune_link) %>%
  st_zm() %>% 
  st_transform(crs = "EPSG:25832")

cph_commune <- filter(dk_commune, KOMKODE %in% study_area_codes)

## Parishes polygons of Denmark, and select those in the study area 
parish_link <- paste(loc_dir, "DAGIREF_SHAPE_UTM32-EUREF89/ADM", "SOGN.shp", sep = "/")
dk_parish  <- read_sf(parish_link) %>%
  st_zm() %>% 
  st_transform(crs = "EPSG:25832")

# Select those where the centroid is in the study area
dk_parish_cent <- st_centroid(dk_parish)
cph_parish_cent <- st_intersection(dk_parish_cent, cph_commune)

cph_parish <- filter(dk_parish, SOGNEKODE %in% cph_parish_cent$SOGNEKODE) %>% 
  # Combine several parish features geometries into one polygon
  group_by(SOGNEKODE, SOGNENAVN) %>% 
  summarise(geometry = st_union(geometry)) %>% 
  ungroup() %>% 
  # add area of the parish (in km2)
  mutate(prsh_area_km2 = as.numeric(units::set_units(st_area(.), km^2)))

## Contour of the study area (merge the parishes in one polygon):
study_area <- cph_parish %>%
  st_union() %>%
  st_sf() %>% 
  st_transform(crs = "EPSG:25832")

```

Some data have been aggregated to gird cells of 100 m x 100 m (i.e. population density), and thus we need to generate a grids over the study area:
```{r grids-100m, cache = TRUE, dependson = "adm-units"}

  grids100m <- study_area %>% 
    # Make regular grids (100m x 100m)
    st_make_grid(cellsize = 100) %>% 
    st_sf() %>% 
    # Name grids as "g001", "g002", ...
    mutate(grid_ID = paste0("g", stringr::str_pad(seq(1, nrow(.), 1), 3, pad = "0")))

```

## BBR data    

From the BBR dataset, we load all buildings for year-round living (*BYG_ANVEND_KODE*):

- 110           = Farmhouse for agricultural property.  
- 120, 121, 122 = Detached single-family house (detached house). 
- 130, 131, 132 = Townhouse, chain, or semi-detached house (vertical separation between the units).
- 140           = Multi-storey residential building (multi-family house, including two-family house (horizontal separation between the units).
- 150           = College.
- 160           = Residential building for residential institution.
- 190           = Second building for year-round living (NOT USED - VERY LOW NUMBER)

```{r load-BBR, cache=TRUE, dependson = "adm-units", cache.extra = file.mtime('C:/Users/FU53VP/OneDrive - Aalborg Universitet/BBR_2021/csv/')}

## Buildings for year round living
  res_codes <- tribble (~BYG_ANVEND_KODE, ~type,
                        110, "Farmhouse",
                        120, "Single-family house",
                        121, "Single-family house",
                        122, "Single-family house",
                        130, "Semi-detached house",
                        131, "Semi-detached house",
                        132, "Semi-detached house",
                        140, "Multi-storey",
                        150, "College", 
                        160, "Residential institution",
                        190, "Second building") %>%
    # Convert type to factor
    mutate(type = factor(type))

## Function for reading residential units from a csv file:
# Read residential units from BBR data 
# Get read residential units in an area
# @param file Link to OneDrive with the data (.csv format)
# @param area Area where we would get the data (default = study_area)

  f_res_units  <- function(.file, .area = study_area) { 
    fread(.file) %>% 
      # Convert to tibble
      as_tibble() %>% 
      # Select only Residential houses - Buildings for year-round living 
      filter(ENH_ANVEND_KODE %in% res_codes$BYG_ANVEND_KODE) %>% 
      # Input empty cells (buildings with only one floor) in Etagebetegn as "st"
      mutate(Etagebetegn = ifelse(Etagebetegn == "", "st", Etagebetegn),
             # Etagebetegn as ordened factor
             Etagebetegn = factor(Etagebetegn, c("k2", "kl", "st", seq(1, 36, 1)),
                                  ordered = TRUE),
             # Group floor levels with 5 or more
             floor_level = fct_other(Etagebetegn,
                                     drop = factor(seq(5, 36)),
                                     other_level = "5 or more")) %>%
      # Add residential description (type) into the dataset
      left_join(res_codes, by = c("ENH_ANVEND_KODE" = "BYG_ANVEND_KODE")) %>% 
      # Convert to sf objects
      st_as_sf(coords = c("etrs89koordinat_ost", "etrs89koordinat_nord"),
               crs = "EPSG:25832") %>% 
      # Get only points in the study area
      mutate(int = st_intersects(., .area) %>% lengths > 0) %>% 
      filter(int == TRUE) %>% 
      # Add year of the BBR dataset  
      mutate(BBR_year = parse_number(stringr::str_extract(.file, "_[0-9]+_")))
  }

## Load all csv files (one file for each year) in the same tibble with a column indicating the year of the dataset: 
# Load residential units 
# NOTE: you may need to change the path to OneDrive - Aalborg Universitet 
# Read all the data together 
  OneDrive_link <- "C:/Users/FU53VP/OneDrive - Aalborg Universitet/BBR_2021/csv/"
  csv_files_link <- list.files(path = OneDrive_link, pattern = "*.csv", full.names = TRUE)
  
  plan(multisession, workers = 3)
  res_units <- future_map_dfr(.x = csv_files_link, .f = f_res_units)
  plan("default")

```

## Population data 

Population data at parish level have been retrieved from [Statistics Denmark](https://www.dst.dk/en) using the R-package [danstat](https://cran.r-project.org/web/packages/danstat/index.html). We have uploaded the following tables:

- [KMSTA001: Population 1. January by parish, ancestry and National Church](https://www.statbank.dk/statbank5a/SelectVarVal/Define.asp?MainTable=KMSTA001&PLanguage=1&PXSId=0&wsid=cftree)
- [KMSTA003: Summary vital statistics by parish and movements](https://www.statbank.dk/statbank5a/SelectVarVal/Define.asp?MainTable=KMSTA003&PLanguage=1&PXSId=0&wsid=cftree)

We have created two auxiliary function for reading the data. `steps` loops by year for getting small pieces of information from the DST API and then putting all together in a data frame. In this sense, we overcome the limitation of the number of rows we can retrieve from the API (i.e. if we call a large dataset we get the error: *Error: API did not return text/csv*). Then, `rm_words` is used in the cleaning process for simplifying the description of some variables we would like to use as columns names. 

```{r aux-funct-DST}

# Loop by year for getting DST data 
  steps <- function(year){
    var_values <- list(id_region, id_ancestry, year)
    var_input <- purrr::map2(.x = var_codes,
                             .y = var_values,
                             .f = ~list(code = .x, values = .y))
    get_data(id_table, variables = var_input)
  }

# Remove punctuation, lowercase, stem, stopwords, and collapse strings
  rm_words <- function(x, stopwords) { x %>% 
      strsplit(" ", fixed = TRUE) %>% 
      lapply(tm::removePunctuation) %>% 
      lapply(tolower) %>% 
      lapply(SnowballC::wordStem) %>% 
      lapply(function(x) x[!x %in% stopwords]) %>% 
      vapply(function(x) paste(x , collapse = "_"), character(1))
  }

```

The code needed for loading the tables are therefore: 

  - KMSTA001:
```{r load-KMSTA001, cache = TRUE, dependson = "adm-units"}

  id_table <- "KMSTA001"
  var_pop <- get_table_metadata(table_id = id_table, variables_only = TRUE)

# Codes for var_input
  var_codes <- c("SOGN", "HERKOMST", "Tid")

# Values for var_input
  # Region: parishes of the study area (i.e. cph_parish)
  id_region <- cph_parish$SOGNEKODE
  # Ancestry
  id_ancestry <- NA
  # Quarters
  id_year <- var_pop$values[[4]]$id   # Select all years
  
# Read data
  prsh_ances_dst <- id_year %>%
    future_map(steps) %>% 
    bind_rows()
  plan("default")

# Clean data 
  prsh_ances <- prsh_ances_dst %>% 
    # Translate column names into English
    rename(parish = SOGN,
           ancestry = HERKOMST,
           date = TID, 
           value = INDHOLD) %>% 
    # Get parish codes (first number of the string)
    mutate(prsh_id = stringr::str_extract(parish, "[[:alnum:]]*")) %>% 
    # Remove the code from the string (prsh)
    mutate(parish = sub("[[:alnum:]]* ", "", parish)) %>% 
    # Get municipality (info inside the parenthesis)
    mutate(prsh_muni = stringr::str_extract(parish, "(?<=\\().*(?=\\))"),
           prsh_muni = gsub(" Municipality", "", prsh_muni),
           # removes white space from start and end of string
           prsh_muni = stringr::str_trim(prsh_muni)) %>% 
    # Get parish name (outside parenthesis) 
    mutate(prsh_name = stringr::str_extract(parish, "(.*(?=\\())"),
           prsh_name = stringr::str_trim(prsh_name)) %>% 
    # Remove duplicate info and order columns 
    select(prsh_id, prsh_name, prsh_muni, date, ancestry, value) %>% 
    # Make shorter names in ancestry
    mutate(ancestry = case_when(
      ancestry == "Persons of Danish origin" ~ "pop_dan",
      ancestry == "Immigrants from western countries" ~ "pop_mi_wst",
      ancestry == "Immigrants from non-western countries" ~ "pop_mi_nwst",
      ancestry == "Descendants from western countries" ~ "pop_de_wst",
      ancestry == "Descendants from non-western countries" ~ "pop_de_nwst"), 
      ancestry = factor(ancestry)) %>% 
    # Pivot (one row for peach parish and year)
    pivot_wider(names_from = ancestry, values_from = value) %>% 
    # Merge immigrants and their descendants (i.e. foreigners) 
    mutate(pop_frgn_wst = pop_mi_wst + pop_de_wst, 
           pop_frgn_nwst = pop_mi_nwst + pop_de_nwst) %>% 
    select(-c(pop_mi_wst, pop_de_wst, pop_mi_nwst, pop_de_nwst)) %>% 
    # Add column with total population
    mutate(pop_total = select(., starts_with("pop_")) %>% rowSums())
  
```

 - KMSTA003:
```{r load-KMSTA003, cache = TRUE, dependson = "adm-units"}

  id_table <- "KMSTA003"
  var_pop <- get_table_metadata(table_id = id_table, variables_only = TRUE)

# Codes for var_input
  var_codes <- c("SOGN", "KIRKEBEV", "Tid")

# Values for var_input
  # Region: all parish
  id_region <- cph_parish$SOGNEKODE
  # Ancestry
  id_movements <- NA
  # Quarters
  id_year <- var_pop$values[[3]]$id   # Select all years

# Read data
  plan(multisession)  
  prsh_stats_dst <- id_year %>%
    future_map(steps) %>% 
    bind_rows()
  plan("default")

# Clean data 
  prsh_stats <- prsh_stats_dst %>%
    # Translate column names into English
    rename(parish = SOGN,
           movements = KIRKEBEV,
           date = TID, 
           value = INDHOLD) %>% 
    # Get parish codes (first number of the string)
    mutate(prsh_id = stringr::str_extract(parish, "[[:alnum:]]*")) %>% 
    # Remove the code from the string (prsh)
    mutate(parish = sub("[[:alnum:]]* ", "", parish)) %>% 
    # Get municipality (info inside the parenthesis)
    mutate(prsh_muni = stringr::str_extract(parish, "(?<=\\().*(?=\\))"),
           prsh_muni = gsub(" Municipality", "", prsh_muni),
           # removes white space from start and end of string
           prsh_muni = stringr::str_trim(prsh_muni)) %>% 
    # Get parish name (outside parenthesis) 
    mutate(prsh_name = stringr::str_extract(parish, "(.*(?=\\())"),
           prsh_name = stringr::str_trim(prsh_name)) %>% 
    # remove duplicate info and order columns 
    select(prsh_id, prsh_name, prsh_muni, date, movements, value) %>% 
    # Clean arguments in movements (remove punctuation, stop-words, stem, and collapse)
    mutate(movements = rm_words(movements, c("in", "the", "of"))) %>%
    # Pivot (one row for each parish and year) 
    pivot_wider(names_from = movements, values_from = value)
  
```

One we have the data we merge both dataset and add the spatial information (note that there are only summary statistics from 2015): 
```{r merge-DST, cache = TRUE, dependson = "adm-units"}

## Merge both tables in one:
  prsh_pop <- prsh_ances %>%  
    full_join(prsh_stats) %>% 
    # remove rows with no summary data (summary data only from 2015 to 2020)
    filter(date >= 2015, date <= 2020)

## Add the spatial information:
  prsh_pop_sf <- cph_parish %>%
    rename(prsh_id = SOGNEKODE) %>% 
    select(prsh_id, prsh_area_km2) %>% 
    left_join(prsh_pop, by = c("prsh_id")) 

```

##  CORINE land use 

Land use have been obtained from CORINE. We downloaded the data from kortforsyningen and used the data from 2010 (*CLC12_DK.shp*).

```{r load-CORINE, cache=TRUE, dependson="adm-units"}

## Create a table with the CORINE land use codes from the EEA
corine_code_link <- "https://www.eea.europa.eu/data-and-maps/data/corine-land-cover-2000-clc2000-250-m-version-9-2007/corine-land-cover-2000-classes-and-rgb-color-codes/clc2000legend.csv/at_download/file"

corine_code <- read_csv(corine_code_link) %>% 
  mutate_if(is.numeric, as.character)
  
## Load shape fine
dangeo_set_param() # Get local directory
corine_link <- paste(loc_dir, "DK_CORINE_SHP_UTM32-WGS84", "CLC12_DK.shp", sep = "/")
dk_corine  <- read_sf(corine_link) %>%
  # Drop z dimension
  st_zm() %>% 
  # Transform coordinates
  st_transform(crs = "EPSG:25832") %>% 
  # Add code description
  left_join(corine_code, by = c("CODE_12" = "CLC_CODE")) 

## Select study area
cph_corine <- st_intersection(dk_corine, study_area) %>% 
  select(CODE_12, LABEL1, LABEL2, LABEL3 , RGB) %>% 
  janitor::clean_names() %>% 
  separate(rgb, c("red", "green", "blue"), sep = "-") %>% 
  mutate(code_12 = as.numeric(code_12),
         hex_col = grDevices::rgb(red, green, blue, max = 255),
         hex_col = fct_reorder(hex_col, code_12),
         label3 = fct_reorder(label3, code_12)) %>% 
  st_sf()
  
```
